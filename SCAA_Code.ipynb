{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaty2Ue9mwKmaHBCgL7Q1K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiPraneethM24/SCAAResearch/blob/main/SCAA_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxpR9nb8IcMK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers scikit-learn imbalanced-learn matplotlib pandas tqdm accelerate seaborn bitsandbytes --quiet\n",
        "\n",
        "import os, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "#Loading of samples\n",
        "dataset_dir = '/kaggle/input/multi-lang-dataset/Dataset'\n",
        "csv_files = [f for f in os.listdir(dataset_dir) if f.endswith('.csv')]\n",
        "df_list = []\n",
        "total_rows = 0\n",
        "max_total_rows = 3000\n",
        "max_per_file = 500\n",
        "\n",
        "print(\"Loading data from CSV files\")\n",
        "for file in tqdm(csv_files, desc=\"Loading CSVs\"):\n",
        "    try:\n",
        "        df_temp = pd.read_csv(os.path.join(dataset_dir, file))\n",
        "        if \"flines\" not in df_temp.columns or \"full_path\" not in df_temp.columns:\n",
        "            continue\n",
        "        df_temp = df_temp.dropna(subset=[\"flines\", \"full_path\"])\n",
        "        df_temp = df_temp.head(max_per_file)\n",
        "        df_list.append(df_temp)\n",
        "        total_rows += len(df_temp)\n",
        "        if total_rows >= max_total_rows:\n",
        "            break\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file}: {e}\")\n",
        "        continue\n",
        "\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "df[\"author\"] = df[\"full_path\"].apply(lambda x: x.split(\"/\")[1] if isinstance(x, str) and \"/\" in x else \"unknown\")\n",
        "print(f\"Loaded {len(df)} code samples across {len(df_list)} files.\")\n",
        "\n",
        "#Using Falcon for obfuscation\n",
        "print(\"Loading Falcon model for obfuscation...\")\n",
        "model_id = \"tiiuae/falcon-rw-1b\"\n",
        "tokenizer_llm = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llm = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "def falcon_obfuscate(code_snippet):\n",
        "    if not isinstance(code_snippet, str) or not code_snippet.strip(): return code_snippet\n",
        "    prompt = f\"Obfuscate the following Java code by renaming variables and adding dead code:\\n\\n{code_snippet}\\n\\nObfuscated Code:\"\n",
        "    try:\n",
        "        inputs = tokenizer_llm(prompt, return_tensors=\"pt\").to(model_llm.device)\n",
        "        outputs = model_llm.generate(**inputs, max_new_tokens=256, pad_token_id=tokenizer_llm.eos_token_id)\n",
        "        result = tokenizer_llm.decode(outputs[0], skip_special_tokens=True)\n",
        "        return result.split(\"Obfuscated Code:\")[-1].strip() if \"Obfuscated Code:\" in result else result\n",
        "    except Exception:\n",
        "        return code_snippet\n",
        "\n",
        "print(\"Obfuscating code\")\n",
        "df_obf = df.copy()\n",
        "df_obf[\"flines\"] = [falcon_obfuscate(code) for code in tqdm(df[\"flines\"], desc=\"Obfuscating\")]\n",
        "df_obf[\"label\"] = 1\n",
        "df_clean = df.copy()\n",
        "df_clean[\"label\"] = 0\n",
        "\n",
        "df_combined = pd.concat([df_clean, df_obf], ignore_index=True)\n",
        "df_combined[\"author_label\"] = df_combined[\"author\"].astype(\"category\").cat.codes\n",
        "\n",
        "#Preparing CodeBert Embeddings\n",
        "print(\"Loading CodeBERT\")\n",
        "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\").to(device)\n",
        "\n",
        "def get_codebert_embeddings(samples, batch_size=8):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(samples), batch_size), desc=\"Embedding with CodeBERT\"):\n",
        "        batch = samples[i:i+batch_size]\n",
        "        try:\n",
        "            tokens = codebert_tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "            with torch.no_grad():\n",
        "                output = codebert_model(**tokens).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            embeddings.append(output)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped batch at {i} due to: {e}\")\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "X = get_codebert_embeddings(df_combined[\"flines\"].tolist())\n",
        "y = df_combined[\"label\"].values\n",
        "yauth = df_combined[\"author_label\"].values\n",
        "\n",
        "#Training of Dual VAE\n",
        "X_clean = X[y == 0]; X_obf = X[y == 1]\n",
        "\n",
        "class VAENet(nn.Module):\n",
        "    def __init__(self, input_dim=768, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(input_dim, 512), nn.ReLU(), nn.Linear(512, 256), nn.ReLU())\n",
        "        self.mu = nn.Linear(256, latent_dim)\n",
        "        self.logvar = nn.Linear(256, latent_dim)\n",
        "        self.decoder = nn.Sequential(nn.Linear(latent_dim, 256), nn.ReLU(), nn.Linear(256, 512), nn.ReLU(), nn.Linear(512, input_dim))\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        return mu + torch.randn_like(std) * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.mu(h), self.logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "def train_vae(model, X_data, epochs=20):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    X_tensor = torch.tensor(X_data).float().to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model(X_tensor)\n",
        "        recon_loss = loss_fn(recon, X_tensor)\n",
        "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        total_loss = recon_loss + kl_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"VAE Epoch [{epoch+1}/{epochs}] Loss: {total_loss.item():.4f}\")\n",
        "    return model\n",
        "\n",
        "print(\"Training VAE (Clean)\")\n",
        "vae_clean = train_vae(VAENet(), X_clean)\n",
        "\n",
        "print(\"Training VAE (Obfuscated)\")\n",
        "vae_obf = train_vae(VAENet(), X_obf)\n",
        "\n",
        "def get_recon_error(model, X_input):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_tensor = torch.tensor(X_input).float().to(device)\n",
        "        recon, _, _ = model(X_tensor)\n",
        "        error = ((X_tensor - recon) ** 2).mean(dim=1).cpu().numpy()\n",
        "    return error\n",
        "\n",
        "err_clean = get_recon_error(vae_clean, X)\n",
        "err_obf = get_recon_error(vae_obf, X)\n",
        "X_hybrid = np.hstack([X, err_clean.reshape(-1,1), err_obf.reshape(-1,1)])\n",
        "\n",
        "#SMOTE Balancing part\n",
        "print(\"SMOTE Balancing...\")\n",
        "sm = SMOTE(random_state=SEED)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_hybrid, y)\n",
        "\n",
        "#Anomaly Detection part\n",
        "print(\"Training Anomaly Detector(RandomForest)\")\n",
        "clf_anomaly = RandomForestClassifier(n_estimators=200, random_state=SEED)\n",
        "clf_anomaly.fit(X_resampled, y_resampled)\n",
        "y_pred_anomaly = clf_anomaly.predict(X_hybrid)\n",
        "\n",
        "print(\"\\nAnomaly Detection Report:\\n\")\n",
        "print(classification_report(y, y_pred_anomaly))\n",
        "sns.heatmap(confusion_matrix(y, y_pred_anomaly), annot=True, cmap=\"Blues\", fmt='d')\n",
        "plt.title(\"Anomaly Detection Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "# === ROC Curve for Anomaly Detection ===\n",
        "# Probability scores for the positive class\n",
        "probs_anomaly = clf_anomaly.predict_proba(X_hybrid)[:, 1]\n",
        "fpr_anomaly, tpr_anomaly, _ = roc_curve(y, probs_anomaly)\n",
        "roc_auc_anomaly = auc(fpr_anomaly, tpr_anomaly)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_anomaly, tpr_anomaly, color='darkorange',\n",
        "         lw=2, label=f\"ROC curve (area = {roc_auc_anomaly:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Anomaly Detection')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve for Authorship Attribution\n",
        "n_classes = len(np.unique(yauth))\n",
        "yauth_bin = label_binarize(yauth, classes=np.arange(n_classes))\n",
        "probs_author = clf_auth.predict_proba(X)\n",
        "colors = sns.color_palette(\"husl\", n_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(yauth_bin[:, i], probs_author[:, i])\n",
        "    auc_score = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'Class {i} (AUC = {auc_score:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Authorship Attribution (Multi-Class)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "# === AUTHORSHIP ATTRIBUTION ===\n",
        "print(\"Training Author Classifier\")\n",
        "clf_auth = RandomForestClassifier(n_estimators=200, random_state=SEED)\n",
        "clf_auth.fit(X, yauth)\n",
        "y_pred_author = clf_auth.predict(X)\n",
        "\n",
        "print(\"\\nAuthorship Attribution Report:\\n\")\n",
        "print(classification_report(yauth, y_pred_author))\n",
        "sns.heatmap(confusion_matrix(yauth, y_pred_author), annot=True, cmap=\"Greens\", fmt='d')\n",
        "plt.title(\"Authorship Attribution Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()"
      ]
    }
  ]
}