{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGkaRi37xt9f"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 0: Install libraries\n",
        "!pip install transformers accelerate scikit-learn tqdm matplotlib --quiet\n",
        "\n",
        "# ✅ STEP 1: Imports\n",
        "import os, glob, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaModel, pipeline\n",
        "\n",
        "# ✅ STEP 2: Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"🖥️ Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\").to(device).eval()\n",
        "llm = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device=-1)\n",
        "\n",
        "# ✅ STEP 3: Load code\n",
        "def load_java_files(base_path, max_files=100):\n",
        "    files = glob.glob(base_path + \"/**/*.java\", recursive=True)\n",
        "    code, authors = [], []\n",
        "    for path in tqdm(files[:max_files], desc=\"📂 Loading .java files\"):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                content = f.read().strip()\n",
        "                if len(content) > 50:\n",
        "                    code.append(content)\n",
        "                    authors.append(os.path.basename(os.path.dirname(path)))\n",
        "        except: continue\n",
        "    return code, authors\n",
        "\n",
        "# ✅ STEP 4: Obfuscation\n",
        "def obfuscate_java_code(snippets):\n",
        "    obf = []\n",
        "    for code in tqdm(snippets, desc=\"🤖 Obfuscating\"):\n",
        "        prompt = f\"You are a code obfuscator. Rename vars, reorder blocks, add dead code. Keep logic.\\nJava Code:\\n{code[:400]}\\nObfuscated Java Code:\"\n",
        "        try:\n",
        "            result = llm(prompt, max_new_tokens=100, do_sample=True, temperature=0.9)[0][\"generated_text\"]\n",
        "            obfuscated = result.split(\"Obfuscated Java Code:\")[-1].strip()\n",
        "        except: obfuscated = code\n",
        "        obf.append(obfuscated)\n",
        "    return obf\n",
        "\n",
        "# ✅ STEP 5: Embedding\n",
        "@torch.no_grad()\n",
        "def get_embeddings(snippets, batch_size=4, max_len=256):\n",
        "    embs = []\n",
        "    for i in tqdm(range(0, len(snippets), batch_size), desc=\"🔗 Embedding\"):\n",
        "        batch = snippets[i:i+batch_size]\n",
        "        try:\n",
        "            tokens = tokenizer(batch, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_len).to(device)\n",
        "            output = model(**tokens).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            embs.extend(output)\n",
        "        except: continue\n",
        "    return np.array(embs, dtype=np.float32)\n",
        "\n",
        "# ✅ STEP 6: VAE Class\n",
        "class BetaVAE(nn.Module):\n",
        "    def __init__(self, input_dim=768, latent_dim=64, beta=4.0):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.encoder = nn.Sequential(nn.Linear(input_dim, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU())\n",
        "        self.mu = nn.Linear(128, latent_dim)\n",
        "        self.logvar = nn.Linear(128, latent_dim)\n",
        "        self.decoder = nn.Sequential(nn.Linear(latent_dim, 128), nn.ReLU(), nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, input_dim))\n",
        "\n",
        "    def reparam(self, mu, logvar):\n",
        "        return mu + torch.randn_like(logvar) * torch.exp(0.5 * logvar)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        mu, logvar = self.mu(enc), self.logvar(enc)\n",
        "        z = self.reparam(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "    def loss_fn(self, x, recon, mu, logvar):\n",
        "        mse = nn.functional.mse_loss(recon, x, reduction='sum')\n",
        "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return mse + self.beta * kl\n",
        "\n",
        "# ✅ STEP 7: Train VAE\n",
        "def train_vae(X, beta=4.0, epochs=10):\n",
        "    vae = BetaVAE(beta=beta).to(device)\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    loader = torch.utils.data.DataLoader(torch.tensor(X, dtype=torch.float32), batch_size=16, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "        total = 0\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            recon, mu, logvar = vae(batch)\n",
        "            loss = vae.loss_fn(batch, recon, mu, logvar)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total += loss.item()\n",
        "        print(f\"📘 Epoch {epoch+1}: Loss = {total:.2f}\")\n",
        "    return vae\n",
        "\n",
        "# ✅ STEP 8: Feature Extraction\n",
        "@torch.no_grad()\n",
        "def extract_features(vae, X):\n",
        "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    recon, mu, logvar = vae(X)\n",
        "    err = ((X - recon)**2).mean(dim=1).cpu().numpy()\n",
        "    return np.hstack([mu.cpu().numpy(), err.reshape(-1,1)]), err\n",
        "\n",
        "# ✅ STEP 9: Evaluate\n",
        "def evaluate_all(features, labels, errors, authors, embeddings):\n",
        "    # 🔍 Anomaly Detection\n",
        "    fpr, tpr, thresholds = roc_curve(labels, errors)\n",
        "    best_idx = np.argmax(tpr - fpr)\n",
        "    threshold = thresholds[best_idx]\n",
        "    pred = (errors > threshold).astype(int)\n",
        "\n",
        "    print(f\"\\n📈 Optimal Anomaly Threshold: {threshold:.4f} (Youden)\")\n",
        "    print(\"📊 Anomaly Detection Report:\\n\", classification_report(labels, pred))\n",
        "    print(\"📉 Confusion Matrix:\\n\", confusion_matrix(labels, pred))\n",
        "\n",
        "    auc_score = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\", color='blue')\n",
        "    plt.plot([0,1],[0,1],'--',color='gray')\n",
        "    plt.title(\"ROC Curve\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
        "    plt.grid(True); plt.legend(); plt.show()\n",
        "\n",
        "    # 🧠 Authorship Attribution\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(authors)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(embeddings, y, stratify=y, test_size=0.2)\n",
        "    clf = RandomForestClassifier(n_estimators=300).fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(f\"\\n🧠 Author Attribution Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "    print(\"📊 Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "    print(\"📉 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ✅ STEP 10: Run Pipeline\n",
        "!unzip -q /content/GPT-Java-GCJ-Dataset-main.zip -d /content/java_data\n",
        "\n",
        "code, authors = load_java_files(\"/content/java_data/GPT-Java-GCJ-Dataset-main\", max_files=100)\n",
        "obf_code = obfuscate_java_code(code)\n",
        "\n",
        "X_clean = get_embeddings(code)\n",
        "X_obf = get_embeddings(obf_code)\n",
        "\n",
        "# Dual-VAE Setup\n",
        "beta_vae = train_vae(X_clean, beta=4.0, epochs=10)\n",
        "features, errors = extract_features(beta_vae, np.vstack([X_clean, X_obf]))\n",
        "labels = np.array([0]*len(X_clean) + [1]*len(X_obf))\n",
        "\n",
        "evaluate_all(features, labels, errors, authors, X_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ STEP 0: Install libraries\n",
        "!pip install transformers accelerate scikit-learn tqdm matplotlib pandas --quiet\n",
        "\n",
        "# ✅ STEP 1: Imports\n",
        "import os, time, glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaModel, pipeline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ STEP 2: Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "# ✅ STEP 3: Load models\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "codebert = RobertaModel.from_pretrained(\"microsoft/codebert-base\").to(device).eval()\n",
        "llm = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device=-1)\n",
        "\n",
        "# ✅ STEP 4: Load multilingual code from GCJ-style CSVs\n",
        "def load_code_from_csvs(folder_path, max_per_file=100):\n",
        "    code, authors = [], []\n",
        "    for file in glob.glob(f\"{folder_path}/*.csv\"):\n",
        "        df = pd.read_csv(file)\n",
        "        df = df[df[\"flines\"].notna() & df[\"full_path\"].notna()]\n",
        "        for _, row in df.iterrows():\n",
        "            path = str(row[\"full_path\"])\n",
        "            snippet = str(row[\"flines\"]).strip()\n",
        "            author = os.path.basename(path.strip().split(\"/\")[0])\n",
        "            if len(snippet) > 50:\n",
        "                code.append(snippet)\n",
        "                authors.append(author)\n",
        "                if len(code) >= max_per_file * len(glob.glob(f\"{folder_path}/*.csv\")):\n",
        "                    break\n",
        "    return code, authors\n",
        "\n",
        "# ✅ STEP 5: Falcon-RW-1B LLM-based obfuscation\n",
        "def obfuscate_code(codes):\n",
        "    obf = []\n",
        "    for c in tqdm(codes, desc=\"Obfuscating\"):\n",
        "        prompt = f\"You are a code obfuscator. Rename vars, reorder blocks, add dead code. Keep logic.\\nJava Code:\\n{c[:200]}\\nObfuscated Java Code:\"\n",
        "        try:\n",
        "            result = llm(prompt, max_new_tokens=200, do_sample=True, temperature=0.9)[0]['generated_text']\n",
        "            obf_code = result.split(\"Obfuscated Java Code:\")[-1].strip()\n",
        "        except:\n",
        "            obf_code = c\n",
        "        obf.append(obf_code)\n",
        "    return obf\n",
        "\n",
        "# ✅ STEP 6: CodeBERT Embeddings\n",
        "@torch.no_grad()\n",
        "def get_embeddings(snippets, batch_size=4, max_len=256):\n",
        "    embs = []\n",
        "    for i in tqdm(range(0, len(snippets), batch_size), desc=\"Embedding\"):\n",
        "        batch = snippets[i:i+batch_size]\n",
        "        try:\n",
        "            tokens = tokenizer(batch, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_len).to(device)\n",
        "            output = codebert(**tokens).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            embs.extend(output)\n",
        "        except: continue\n",
        "    return np.array(embs, dtype=np.float32)\n",
        "\n",
        "# ✅ STEP 7: Beta-VAE\n",
        "class BetaVAE(nn.Module):\n",
        "    def __init__(self, input_dim=768, latent_dim=64, beta=4.0):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.encoder = nn.Sequential(nn.Linear(input_dim, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU())\n",
        "        self.mu = nn.Linear(128, latent_dim)\n",
        "        self.logvar = nn.Linear(128, latent_dim)\n",
        "        self.decoder = nn.Sequential(nn.Linear(latent_dim, 128), nn.ReLU(), nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, input_dim))\n",
        "    def reparam(self, mu, logvar):\n",
        "        return mu + torch.randn_like(logvar) * torch.exp(0.5 * logvar)\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.mu(h), self.logvar(h)\n",
        "        z = self.reparam(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "    def loss_fn(self, x, recon, mu, logvar):\n",
        "        mse = nn.functional.mse_loss(recon, x, reduction='sum')\n",
        "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return mse + self.beta * kl\n",
        "\n",
        "def train_vae(X, beta=4.0, epochs=10):\n",
        "    vae = BetaVAE(beta=beta).to(device)\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    loader = torch.utils.data.DataLoader(torch.tensor(X, dtype=torch.float32), batch_size=16, shuffle=True)\n",
        "    for ep in range(epochs):\n",
        "        total = 0\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            recon, mu, logvar = vae(batch)\n",
        "            loss = vae.loss_fn(batch, recon, mu, logvar)\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            total += loss.item()\n",
        "        print(f\"Epoch {ep+1}, Loss: {total:.2f}\")\n",
        "    return vae\n",
        "\n",
        "# ✅ STEP 8: Extract features\n",
        "@torch.no_grad()\n",
        "def extract_features(vae, X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    recon, mu, _ = vae(X_tensor)\n",
        "    errors = ((X_tensor - recon)**2).mean(dim=1).cpu().numpy()\n",
        "    return np.hstack([mu.cpu().numpy(), errors.reshape(-1,1)]), errors\n",
        "\n",
        "# ✅ STEP 9: Evaluation\n",
        "def evaluate_all(features, labels, errors, authors, embeddings):\n",
        "    fpr, tpr, thresholds = roc_curve(labels, errors)\n",
        "    best_idx = np.argmax(tpr - fpr)\n",
        "    threshold = thresholds[best_idx]\n",
        "    pred = (errors > threshold).astype(int)\n",
        "\n",
        "    print(f\"\\nAnomaly Detection (Threshold={threshold:.4f}):\")\n",
        "    print(classification_report(labels, pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(labels, pred))\n",
        "\n",
        "    auc_score = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\", color='blue')\n",
        "    plt.plot([0,1],[0,1],'--',color='gray')\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curve\")\n",
        "    plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "    # Authorship Attribution\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(authors)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(embeddings, y, stratify=y, test_size=0.2)\n",
        "    clf = RandomForestClassifier(n_estimators=300, max_depth=20)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\"\\nAuthor Attribution:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ✅ STEP 10: Run Full Pipeline\n",
        "csv_folder = \"/content/drive/MyDrive/Dataset1\"  # Your folder with gcj2017.csv etc.\n",
        "\n",
        "code, authors = load_code_from_csvs(csv_folder, max_per_file=100)\n",
        "obf_code = obfuscate_code(code)\n",
        "\n",
        "X_clean = get_embeddings(code)\n",
        "X_obf = get_embeddings(obf_code)\n",
        "\n",
        "vae_clean = train_vae(X_clean, beta=4.0, epochs=10)\n",
        "X_all = np.vstack([X_clean, X_obf])\n",
        "y_bin = np.array([0]*len(X_clean) + [1]*len(X_obf))\n",
        "\n",
        "features, errors = extract_features(vae_clean, X_all)\n",
        "evaluate_all(features, y_bin, errors, authors, X_clean)\n"
      ],
      "metadata": {
        "id": "G-ikMLVjx04u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}